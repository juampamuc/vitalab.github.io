---
layout: review
title: "Domain Generalization via Model-Agnostic Learning of Semantic Features"
tags: domain-adaptation
author: "Pierre-Marc Jodoin"
cite:
    authors: "Qi Dou, Daniel C. Castro, Konstantinos Kamnitsas, Ben Glocker"
    title:   "Domain Generalization via Model-Agnostic Learning of Semantic Features"
    venue:   "NeurIPS 2019"
pdf: "https://arxiv.org/pdf/1910.13580.pdf"
---


![](/article/images/MASF/sc01.jpg)


# Highlights
This paper propose a new domain generalization method.  The goal is for a model, trained on multi-domain source data, to generalize well on target domains with unknown statistics.  The proposed system is illustred in Figigure 1.

# Methods

Say we have various domains $$D=\{D_1,D_2,...,D_K\}$$ and in each domain we have a series of labeled data $$D_k=\{(x_n^k,y_n^k)\}_{n=1}^{N_k}$$.  The proposed system comes with a feature extractor $$F_\phi$$ and a task network $$T_\theta$$.  The predicted output for a given input $$x$$ is $$\hat{y}=softmax(T_\theta(F_\phi(x)))$$. 

At each iteration, $$D$$ is split into **meta-train** $$D_{tr}$$ and **meta-test** $$D_{te}$$ domains. Then, as shown in Figure 1, 3 losses are being optimised.

* A *task* loss called $${\cal L}_{task}$$  (a basic cross-entropy). 
* A *global* loss $${\cal L}_{global}$$
* A *local* loss $${\cal L}_{local}$$

The goal of the *global* loss  is to make sure that the class relationship is the same across domain. This is done by computing the average latent vector for every data associated to each class $$c$$ and each domain $$k$$

![](/article/images/MASF/sc02.jpg)

Then, take the average latent vector, decode it and compute it *soft*-crossentropy

![](/article/images/MASF/sc03.jpg)

where $$\tau$$ is a temperature term.  The global loss is the following Jensen-Shannon divergence

![](/article/images/MASF/sc04.jpg)


As for the *local* loss, its goal is to enforce that a latent vector is closer to another latent vectir from the same class than one from from other classes.  For this, they propose a **contractive loss** or a **triplet loss**

![](/article/images/MASF/sc05.jpg)
![](/article/images/MASF/sc06.jpg)


The overall training algorithm goes as follows

![](/article/images/MASF/sc07.jpg)



# Results


The authors report good recognition accuracies on the PACS dataset (a dataset with challenging domain shift).  Notice that **DeepAll** is their baseline for which all domains are merged and $$F_\phi.T_\theta$$ is trained by standard supervised learning with $${\cal L}_{task}$$.  Interestingly, DeepAll beats several SOTA methods!


![](/article/images/MASF/sc08.jpg)
![](/article/images/MASF/sc09.jpg)


Unfortunately, their method does not seem to work on medical images

![](/article/images/MASF/sc10.jpg)


Code is available here: [https://github.com/biomedia-mira/masf](https://github.com/biomedia-mira/masf)

