---
layout: review
title: "A Distributional Perspective on Reinforcement Learning"
tags: reinforcement
author: "Antoine Théberge"
cite:
    authors: "Marc G. Bellemare, Will Dabney, Rémi Munos"
    title:   "A Distributional Perspective on Reinforcement Learning"
    venue:   "PMLR 2017"
pdf: "https://arxiv.org/abs/1707.06887"
---


# Highlights

- Proposes a distributional perspective on reinforcement learning
- Achieves state-of-the-art results of the Atari 2600 suite of games

# Introduction

Reinforcement learning (RL) typically tries to maximise the expected value ($$Q$$) of the current and future states, as defined by the Bellman Equation:

$$Q(x,a) = \mathbb{E}[R(x,a) + {\gamma}Q(X',A')], x \sim X, a \sim A.$$ [^1]

However, the expectation might not encompass the true distribution of possible returns from a state, for example if one action might lead to disastrous rewards and another to desirable rewards. The expectation, which would fall in the middle, would actually never be encountered by the agent.

# Methods

## Theory

The authors instead propose to learn policies according to the *distributional Bellman Equation*, defined as

$$Z(x,a) \stackrel{D}{=} R(x,a) + \gamma Z(X', A'),$$

with

$$Q(x,a) = \mathbb{E}[Z(X,A)].$$

This last line reinforces that the policy still tries to maximize $$Q$$, e.g. the expeted return. However, the authors propose to learn the full distribution of possible returns, instead of just the expectation. The authors argue that this allows to preserve the multi-modality of the return distribution, which leads to a more stable learning process and that it mitigates the effects of learning from a non-stationary policy.

For the policy evaluation phase, RL tries to find the true utility function $$Q^\pi$$ that exactly approximates the expected return for the policy $$\pi$$. To do so, the distance between the prediction of $$Q_\theta$$ and the target, which acts as a partial label, is typically minimized according to

$$\mathcal{L}_{x,a}(\theta) = dist(\underbrace{Q_\theta(x,a)}_\text{prediction}, \underbrace{r(x,a) + {\gamma}Q_\theta(x',a')}_\text{target}).$$

The distance is usually the mean squared error, leading to the following loss:

$$\mathcal{L}_{x,a}(\theta) = (r(x,a) + {\gamma}Q_\theta(x',a')  - Q_\theta(x,a))^2.$$

As such, there needs to be a distance to minimize s.t.

$$\mathcal{L}_{x,a}(\theta) = dist(\underbrace{Z_\theta(X, A)}_\text{prediction}, \underbrace{r(x,a) + {\gamma}Z_\theta(X',A')}_\text{target}),$$

when minimized, leads to $$Z_{\theta}(x,a) = Z^{\pi}(x,a)$$. One option for the distance function would be the Kullback–Leibler (KL) divergence:

For two distributions $$p$$ and $$q$$ with the *same support*, the KL divergence (in the discrete case) corresponds to

$$ KL(p||q) = \sum_{i=1}^N p(x_i)\log \frac{p(x_i)}{q(x_i)}. $$

However, the authors argue that since the predicted and target distributions do not share the same support, the KL divergence would provide an infinite distance between the predicted and target distributions. 

Instead, the authors suggest that the first Wasserstein metric can be used instead as a distance measure to compare the target and predicted distributions.

## Practice

### Data


# Results


# Conclusions


# Remarks


# References

[^1]: It is somewhat common to use $$x$$ instead of $$s$$ to denote states.
