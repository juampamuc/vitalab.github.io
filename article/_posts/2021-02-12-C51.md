---
layout: review
title: "A Distributional Perspective on Reinforcement Learning"
tags: reinforcement
author: "Antoine Théberge"
cite:
    authors: "Marc G. Bellemare, Will Dabney, Rémi Munos"
    title:   "A Distributional Perspective on Reinforcement Learning"
    venue:   "PMLR 2017"
pdf: "https://arxiv.org/abs/1707.06887"
---


# Highlights

- Proposes a distributional perspective on reinforcement learning
- Achieves state-of-the-art results of the Atari 2600 suite of games

# Introduction

Reinforcement learning (RL) typically tries to maximise the expected value ($$Q$$) of the current and future states, as defined by the Bellman Equation:

$$Q(x,a) = \mathbb{E}[R(x,a) + {\gamma}Q(X',A')], x \sim X, a \sim A.$$ [^1]

However, the expectation might not encompass the true distribution of possible returns from a state, for example if one action might lead to disastrous rewards and another to desirable rewards. The expectation, which would fall in the middle, would actually never be encountered by the agent.

# Methods

The authors instead propose to learn policies according to the *distributional Bellman Equation*, defined as

$$Z(x,a) \stackrel{D}{=} R(x,a) + \gamma Z(X', A'),$$

with

$$Q(x,a) = \mathbb{E}[Z(X,A)].$$

This last line reinforces that the policy still tries to maximize $$Q$$. However, the authors propose to learn the full distribution of possible returns, instead of just the expectation. The authors argue that this allows to preserve the multi-modality of the return distribution, which leads to a more stable learning process and that it mitigates the effects of learning from a non-stationary policy.

## Preliminaries

For the policy evaluation phase, RL typically tries to minimize the following loss:

$$\mathcal{L}_{x,a}(\theta) = (r(x,a) + {\gamma}Q_\theta(x,a)  - Q_\theta(x',a'))^2.$$


## Data


# Results


# Conclusions


# Remarks


# References

[^1]: It is somewhat common to use $$x$$ instead of $$s$$ to denote states.
