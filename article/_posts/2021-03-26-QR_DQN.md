---
layout: review
title: "Distributional Reinforcement Learning with Quantile Regression"
tags: reinforcement
author: "Antoine Théberge"
cite:
    authors: "Will Dabney, Mark Rowland, Marc G. Bellemare, Rémi Munos"
    title:   "Distributional Reinforcement Learning with Quantile Regression"
    venue:   "AAAI 2018"
pdf: "https://arxiv.org/pdf/1710.10044.pdf"
---


# Highlights

- Closes the gap between the theory and practice on distributional reinforcement learning
- Achieves state-of-the-art results of the Atari 2600 suite of games

# Introduction

[Distributional Reinforcement Learning](https://vitalab.github.io/article/2021/02/12/C51.html) has been proposed as an alternative to standard reinforcement learning where the whole distribution of future rewards is learned instead of its expectation:

$$Z(x,a) \stackrel{D}{=} R(x,a) + \gamma Z(X', A'),$$

where $$Z(x,a) \in \mathbb{R}^{\vert A \vert \times N}$$ a set of atoms forming the [probability mass function (PMF)](https://en.wikipedia.org/wiki/Probability_mass_function) of the return distribution. The goal then became to minimize the distance between $$Z_\theta(x,a)$$ and $$R(x,a) + \gamma Z_\theta(X',A')$$. While the [Wasserstein metric](https://en.wikipedia.org/wiki/Wasserstein_metric) could be used in theory to act as a distance metric between the two distributions, it could not in practice be used as a loss function to be minimized by stochastic gradient descent.

# Methods

## The Wasserstein distance

![](/article/images/qr_dqn/fig1.jpeg)

The $$p$$-Wasserstein distance $$d_p$$ is the $$L^p$$ metric on inverse [cummulative distribution functions](https://en.wikipedia.org/wiki/Cumulative_distribution_function) (inverse CDFs) $$F^{-1}$$ between two distribution functions $$U$$ and $$Y$$:


$$\begin{aligned}
d_p(U,Y) &= \Big(\int_0^1 \vert F^{-1}_Y(\omega) - F^{-1}_U(\omega) \vert^p d\omega \Big)^{\frac{1}{p}} \\
F^{-1}_Y(w) &= \text{inf}\{y \in \mathbb{R}: \omega \leq F_Y(y)\} \; \text{// This is the definition of a quantile.} \\ 
F_Y(y) &= Pr(Y \leq t)
\end{aligned}$$


Figure 2 allows to visualize the 1-Wasserstein metric.

## QR-DQN

To bridge the gap between theory and practice, the authors reformulate the atoms forming $$Z(X,A)$$ to instead be the *medians of $$N$$ quantiles of the distribution* instead of the weights of $$N$$ possible values. As such, instead of learning the weights of fixed-value atoms, the authors propose to learn the value of fixed-weight atoms $$z_i$$.

Formally, $$Z_\theta$$ becomes a __quantile distribution__ defined as

$$Z_\theta(x,a) = \frac{1}{N}\sum_1^N \delta_{\theta_{i}(x,a)},$$

with $$\delta$$ a Dirac function.

> Compared to the original parametrization, the benefits of a parameterized quantile distribution are threefold. First, (1) **we are not restricted to prespecified bounds on the support**, or a uniform resolution, potentially leading to significantly more accurate predictions when the range of returns vary greatly across states. This also (2) lets us **do away with the unwieldy projection step present in C51**, as there are no issues of disjoint supports. Together, these obviate the need for domain knowledge about the bounds of the return distribution when applying the algorithm to new tasks. Finally, (3) this reparametrization **allows us to minimize the Wasserstein loss**, without suffering from biased gradients, specifically, **using quantile regression**.

Quantile regression[^1] is a method for approximating the quantile function of a distribution. The quantile regression loss $$\mathcal{L}^\tau$$ for a quantile $$\tau \in [0, 1]$$ is a loss function that penalizes overestimation errors with weight $$\tau$$ and underestimation errors with weight $$1-\tau$$. It can be minimized by stochastic gradient descent and is defined as

$$\mathcal{L}^\tau(\theta) = \mathbb{E}_{\hat{Z} \sim Z}[{\rho_\tau}(\hat{Z} - \theta)],$$

where $$\rho_{\tau}(u)$$ can be defined as

$$\rho_{\tau}(u) = \begin{cases} u(1-\tau) \; &\text{if} \; u < 0 \\
                                 u\tau\; &\text{if} \; u \geq 0 \end{cases}.$$

In practice, the authors use a Huber loss version of the above loss. The authors still base their algorithm on DQN, and the algorithm for the quantile update is given below:

![](/article/images/qr_dqn/alg1.jpeg)

## Data

To compare themselves against DQN and C51[^2], the authors used the Arcade Learning Environment.


# Results

![](/article/images/qr_dqn/fig4.jpeg)
![](/article/images/qr_dqn/tab1.jpeg)

# Conclusions

QR-DQN bridges the gap between theory and practice and brings significant performance improvements over C51 and DQN.

# Remarks


# References

[^1]: [https://en.wikipedia.org/wiki/Quantile_regression](https://en.wikipedia.org/wiki/Quantile_regression)
[^2]: [https://vitalab.github.io/article/2021/02/12/C51.html](https://vitalab.github.io/article/2021/02/12/C51.html)
